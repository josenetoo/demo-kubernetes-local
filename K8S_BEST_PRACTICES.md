# üéØ Kubernetes - Boas Pr√°ticas e Checklists

> Guia de boas pr√°ticas e checklists de implementa√ß√£o para componentes Kubernetes

---

## √çndice

1. [StatefulSet](#1-statefulset)
2. [Probes (Health Checks)](#2-probes-health-checks)
3. [Secret](#3-secret)
4. [PersistentVolume (PV) e PersistentVolumeClaim (PVC)](#4-persistentvolume-pv-e-persistentvolumeclaim-pvc)
5. [HorizontalPodAutoscaler (HPA)](#5-horizontalpodautoscaler-hpa)
6. [Deploy Completo](#6-deploy-completo)

---

## 1. StatefulSet

### ‚öôÔ∏è Boas Pr√°ticas

```yaml
# ‚úÖ FA√áA
- Use Headless Service (clusterIP: None)
- Configure readinessProbe (ordem depende de ready)
- Use volumeClaimTemplates para storage
- Configure PodDisruptionBudget
- Implemente backup dos PVCs
- Use init containers para configura√ß√£o

# ‚ùå N√ÉO FA√áA
- Usar para aplica√ß√µes stateless
- Deletar PVCs sem backup
- Escalar rapidamente (respeite a ordem)
- Usar storage compartilhado entre Pods
- Ignorar ordem de inicializa√ß√£o
```

### üìã Checklist de Implementa√ß√£o

```bash
# 1. Criar Headless Service
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
    - port: 3306

# 2. Criar StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi

# 3. Verificar cria√ß√£o sequencial
kubectl get pods -w
# Aguarde mysql-0 Ready antes de mysql-1

# 4. Testar DNS est√°vel
kubectl run -it --rm debug --image=busybox -- nslookup mysql-0.mysql

# 5. Verificar PVCs criados
kubectl get pvc
# data-mysql-0, data-mysql-1, data-mysql-2
```

---

## 2. Probes (Health Checks)

### ‚öôÔ∏è Boas Pr√°ticas

```yaml
# ‚úÖ FA√áA
- Use Readiness em TODOS os Pods que recebem tr√°fego
- Liveness apenas se necess√°rio (evita restart loops)
- initialDelaySeconds > tempo de startup da app
- periodSeconds: Readiness (5-10s), Liveness (10-30s)
- failureThreshold: 3 (padr√£o razo√°vel)
- Endpoint /health leve e r√°pido (<1s)

# ‚ùå N√ÉO FA√áA
- Liveness muito agressivo (periodSeconds muito baixo)
- Readiness sem Liveness (Pod travado fica sem tr√°fego para sempre)
- Endpoint /health pesado (consultas ao DB)
- initialDelaySeconds muito baixo (falsos positivos)
- Usar mesmos valores para Liveness e Readiness
```

### üìã Checklist de Implementa√ß√£o

```bash
# 1. Crie endpoint /health na aplica√ß√£o
GET /health ‚Üí 200 OK (saud√°vel) ou 500 (problema)

# 2. Configure Readiness (essencial)
readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 2
  periodSeconds: 5

# 3. Configure Liveness (se necess√°rio)
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 3

# 4. Teste as probes
kubectl describe pod <pod-name>
# Verifique eventos: Readiness/Liveness probe failed

# 5. Monitore restarts
kubectl get pods
# RESTARTS deve ser 0 ou baixo
```

### üéØ Valores Recomendados por Tipo de Aplica√ß√£o

```yaml
# Aplica√ß√£o Web (Node.js, Python, Ruby)
readinessProbe:
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

livenessProbe:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Aplica√ß√£o Java (Spring Boot)
readinessProbe:
  initialDelaySeconds: 30  # JVM startup lento
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

livenessProbe:
  initialDelaySeconds: 60  # JVM startup lento
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3

# Banco de Dados (MySQL, PostgreSQL)
readinessProbe:
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

livenessProbe:
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 5  # Mais tolerante
```

---

## 3. Secret

### üîí Boas Pr√°ticas de Seguran√ßa

```bash
# ‚úÖ FA√áA
- Use RBAC para limitar acesso a Secrets
- Habilite encryption at rest no etcd
- Use ferramentas externas (Vault, Sealed Secrets)
- Rotacione Secrets regularmente
- Use stringData (converte automaticamente para base64)
- Limite Secrets por namespace
- Use imagePullSecrets para registries privados

# ‚ùå N√ÉO FA√áA
- Commitar Secrets no Git
- Logar valores de Secrets
- Expor Secrets em respostas HTTP
- Usar Secrets para dados n√£o-sens√≠veis
```

### üìã Checklist de Implementa√ß√£o

```bash
# 1. Criar Secret via kubectl
kubectl create secret generic my-secret \
  --from-literal=password=s3cr3t \
  --from-file=ssh-key=~/.ssh/id_rsa

# 2. Ou via YAML (use stringData)
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
stringData:
  password: s3cr3t
  api-key: abc123

# 3. Verificar Secret criado
kubectl get secret my-secret
kubectl describe secret my-secret

# 4. Usar no Pod
env:
  - name: PASSWORD
    valueFrom:
      secretKeyRef:
        name: my-secret
        key: password

# 5. Testar no Pod
kubectl exec <pod> -- env | grep PASSWORD
```

### üîê Ferramentas Recomendadas

```bash
# Sealed Secrets (Bitnami)
# Criptografa Secrets para commit no Git
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.0/controller.yaml

# External Secrets Operator
# Sincroniza com Vault, AWS Secrets Manager, etc.
helm install external-secrets external-secrets/external-secrets

# SOPS (Mozilla)
# Criptografa arquivos YAML com chaves KMS
sops -e secret.yaml > secret.enc.yaml
```

---

## 4. PersistentVolume (PV) e PersistentVolumeClaim (PVC)

### ‚öôÔ∏è Boas Pr√°ticas

```yaml
# ‚úÖ FA√áA
- Use StorageClass para provisionamento din√¢mico
- Configure backups regulares dos PVs
- Use Retain policy em produ√ß√£o
- Monitore uso de storage
- Configure limites de storage por namespace
- Use ReadWriteOnce quando poss√≠vel (melhor performance)
- Teste restore de backups regularmente

# ‚ùå N√ÉO FA√áA
- Usar hostPath em produ√ß√£o
- Deletar PVCs sem backup
- Compartilhar PVC entre Deployments (use StatefulSet)
- Ignorar limites de storage
- Usar Delete policy sem backup
- Assumir que todos storages suportam RWX
```

### üìã Checklist de Implementa√ß√£o

```bash
# 1. Verificar StorageClass dispon√≠vel
kubectl get sc

# 2. Criar PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi

# 3. Verificar bind
kubectl get pvc my-pvc
# STATUS deve ser Bound

# 4. Usar no Pod/Deployment
volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc

# 5. Testar persist√™ncia
kubectl exec -it <pod> -- sh -c "echo test > /mnt/data/file.txt"
kubectl delete pod <pod>
# Novo pod deve ter o arquivo
```

### üíæ Estrat√©gias de Backup

```bash
# Velero (backup completo do cluster)
velero install --provider aws --bucket my-backup-bucket

# Backup de PVC espec√≠fico
velero backup create my-backup --include-resources pvc,pv

# Restore
velero restore create --from-backup my-backup

# Snapshot de PV (AWS EBS)
kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: my-snapshot
spec:
  volumeSnapshotClassName: csi-aws-vsc
  source:
    persistentVolumeClaimName: my-pvc
EOF

# Verificar snapshot
kubectl get volumesnapshot
```

---

## 5. HorizontalPodAutoscaler (HPA)

### üéØ Boas Pr√°ticas

```yaml
# ‚úÖ FA√áA
- Defina minReplicas >= 2 (alta disponibilidade)
- Configure requests/limits adequados
- Use m√∫ltiplas m√©tricas (CPU + mem√≥ria)
- Configure PodDisruptionBudget
- Monitore m√©tricas customizadas
- Teste carga antes de produ√ß√£o
- Configure cooldown adequado

# ‚ùå N√ÉO FA√áA
- minReplicas = 1 (sem redund√¢ncia)
- Sem resources definidos
- Alvos muito agressivos (ex: 90% CPU)
- Esquecer cooldown (flapping)
- HPA + escala manual simult√¢nea
```

### üìã Checklist de Implementa√ß√£o

```bash
# 1. Verificar metrics-server instalado
kubectl get deployment metrics-server -n kube-system

# 2. Configurar resources no Deployment
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 512Mi

# 3. Criar HPA
kubectl autoscale deployment my-app \
  --cpu-percent=50 \
  --min=2 \
  --max=10

# Ou via YAML
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

# 4. Verificar HPA
kubectl get hpa
kubectl describe hpa my-app

# 5. Testar autoscaling
# Gerar carga
kubectl run -it --rm load-generator --image=busybox -- /bin/sh -c "while true; do wget -q -O- http://my-app; done"

# Monitorar escala
kubectl get hpa -w
```

### üìä Valores Recomendados

```yaml
# Aplica√ß√£o Web (tr√°fego vari√°vel)
minReplicas: 3
maxReplicas: 20
targetCPUUtilizationPercentage: 60
targetMemoryUtilizationPercentage: 70

# API Backend (tr√°fego previs√≠vel)
minReplicas: 2
maxReplicas: 10
targetCPUUtilizationPercentage: 70

# Worker/Job Processor (carga em lote)
minReplicas: 1
maxReplicas: 50
targetCPUUtilizationPercentage: 80

# Microservi√ßo Cr√≠tico
minReplicas: 5
maxReplicas: 30
targetCPUUtilizationPercentage: 50
```

---

## 6. Deploy Completo

### üìã Checklist de Deploy Completo

```bash
# 1. Configura√ß√£o
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml

# 2. Workload
kubectl apply -f k8s/deployment.yaml

# 3. Rede
kubectl apply -f k8s/service.yaml

# 4. M√©tricas (se necess√°rio)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 5. Autoscaling
kubectl apply -f k8s/hpa.yaml

# 6. Verifica√ß√£o
kubectl get all -l app=demo-app
kubectl get cm,secret
kubectl get hpa
```

### üîç Verifica√ß√£o P√≥s-Deploy

```bash
# 1. Verificar Pods rodando
kubectl get pods
# STATUS deve ser Running

# 2. Verificar logs
kubectl logs -f deployment/my-app

# 3. Verificar Service
kubectl get svc my-app
kubectl get endpoints my-app

# 4. Testar conectividade
kubectl run test --rm -it --image=busybox -- wget -qO- http://my-app

# 5. Verificar Probes
kubectl describe pod <pod-name> | grep -A 5 "Liveness\|Readiness"

# 6. Verificar HPA
kubectl get hpa
# TARGETS deve mostrar m√©tricas (ex: 20%/50%)

# 7. Verificar eventos
kubectl get events --sort-by='.lastTimestamp'
```

### üö® Troubleshooting Comum

```bash
# Pod em CrashLoopBackOff
kubectl logs <pod-name> --previous
kubectl describe pod <pod-name>

# Service n√£o responde
kubectl get endpoints <service-name>
# Verificar se h√° endpoints

# HPA n√£o escala
kubectl describe hpa <hpa-name>
# Verificar se metrics-server est√° funcionando
kubectl top pods

# PVC em Pending
kubectl describe pvc <pvc-name>
# Verificar se h√° StorageClass dispon√≠vel

# Secret n√£o encontrado
kubectl get secret <secret-name>
kubectl describe pod <pod-name> | grep -A 5 "Events"
```

---

## üéì Resumo de Boas Pr√°ticas Gerais

### üîí Seguran√ßa
- Use RBAC para controle de acesso
- Criptografe Secrets at rest
- N√£o exponha portas desnecess√°rias
- Use NetworkPolicies
- Escaneie imagens por vulnerabilidades

### üìä Observabilidade
- Configure logs centralizados
- Use Prometheus + Grafana
- Configure alertas
- Monitore recursos (CPU, mem√≥ria, storage)
- Use tracing distribu√≠do (Jaeger, Zipkin)

### üöÄ Performance
- Configure resources (requests/limits)
- Use HPA para autoscaling
- Configure PodDisruptionBudget
- Use readiness/liveness probes
- Otimize imagens Docker

### üíæ Storage
- Use StorageClass din√¢mico
- Configure backups regulares
- Monitore uso de storage
- Use Retain policy em produ√ß√£o
- Teste restore regularmente

### üîÑ CI/CD
- Use GitOps (ArgoCD, Flux)
- Automatize testes
- Use ambientes separados (dev, staging, prod)
- Implemente rollback autom√°tico
- Use canary/blue-green deployments

---

## üìö Recursos Adicionais

- [Kubernetes Best Practices (Google)](https://cloud.google.com/architecture/best-practices-for-running-kubernetes)
- [Production Best Practices (Kubernetes Docs)](https://kubernetes.io/docs/setup/best-practices/)
- [12 Factor App](https://12factor.net/)
- [CNCF Landscape](https://landscape.cncf.io/)
